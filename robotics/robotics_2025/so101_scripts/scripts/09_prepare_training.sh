#!/bin/bash
################################################################################
# Script: 09_prepare_training.sh
# Purpose: Generate training commands and documentation
# Usage: ./scripts/09_prepare_training.sh
################################################################################

set -euo pipefail

# Script metadata
SCRIPT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
TIMESTAMP="$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${SCRIPT_DIR}/logs/09_prepare_training_${TIMESTAMP}.log"
OUTPUT_FILE="${SCRIPT_DIR}/TRAINING_COMMANDS.md"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $*" | tee -a "$LOG_FILE"
}

log_info() {
    echo -e "${BLUE}[INFO]${NC} $*" | tee -a "$LOG_FILE"
}

# Print header
echo ""
echo "=============================================="
echo "  Prepare Training Commands"
echo "=============================================="
echo ""
log "Generating training documentation..."
log "Log file: $LOG_FILE"
echo ""

# Load configuration
source "$SCRIPT_DIR/.env"
source "$SCRIPT_DIR/config/dataset_config.sh"

# Generate training commands file
cat > "$OUTPUT_FILE" << EOF
# Training Commands for ${DATASET_NAME}

Generated on: $(date)

## Dataset Information

- **Repository**: \`${DATASET_REPO_ID}\`
- **Task**: ${DATASET_TASK}
- **Episodes**: ${DATASET_NUM_EPISODES}
- **Episode Duration**: ${DATASET_EPISODE_TIME}s

## Training on Cloud (MI300X)

### 1. ACT (Action Chunking Transformer)

\`\`\`bash
lerobot-train \\
  --dataset.repo_id=${DATASET_REPO_ID} \\
  --policy.type=act \\
  --batch_size=64 \\
  --steps=10000 \\
  --output_dir=outputs/train/act_${DATASET_NAME} \\
  --job_name=act_${DATASET_NAME} \\
  --policy.device=cuda \\
  --wandb.enable=true \\
  --policy.push_to_hub=false
\`\`\`

### 2. SmolVLA (Vision-Language-Action)

First install VLA dependencies:
\`\`\`bash
cd ~/lerobot
pip install -e ".[smolvla]"
\`\`\`

Then train:
\`\`\`bash
lerobot-train \\
  --dataset.repo_id=${DATASET_REPO_ID} \\
  --policy.type=smolvla \\
  --batch_size=32 \\
  --steps=10000 \\
  --output_dir=outputs/train/smolvla_${DATASET_NAME} \\
  --job_name=smolvla_${DATASET_NAME} \\
  --policy.device=cuda \\
  --wandb.enable=true \\
  --policy.push_to_hub=false
\`\`\`

### 3. Pi (Diffusion Policy)

First install Pi dependencies:
\`\`\`bash
cd ~/lerobot
pip install -e ".[pi]"
\`\`\`

Then train:
\`\`\`bash
lerobot-train \\
  --dataset.repo_id=${DATASET_REPO_ID} \\
  --policy.type=pi \\
  --batch_size=64 \\
  --steps=10000 \\
  --output_dir=outputs/train/pi_${DATASET_NAME} \\
  --job_name=pi_${DATASET_NAME} \\
  --policy.device=cuda \\
  --wandb.enable=true \\
  --policy.push_to_hub=false
\`\`\`

## Upload Trained Model

After training, upload the checkpoint to HuggingFace:

\`\`\`bash
huggingface-cli upload ${HF_USER}/act_${DATASET_NAME} \\
  outputs/train/act_${DATASET_NAME}/checkpoints/last/pretrained_model
\`\`\`

## Inference on Edge

Download the trained model and run inference:

\`\`\`bash
# The model will be downloaded automatically when you specify the path
lerobot-record \\
  --robot.type=so101_follower \\
  --robot.port=/dev/ttyACM1 \\
  --robot.id=my_awesome_follower_arm \\
  --robot.cameras="{top: {type: opencv, index_or_path: 8, width: 640, height: 480, fps: 30}, side: {type: opencv, index_or_path: 6, width: 640, height: 480, fps: 30}}" \\
  --dataset.single_task="${DATASET_TASK}" \\
  --dataset.repo_id=${HF_USER}/eval_${DATASET_NAME} \\
  --dataset.root=\${PWD}/eval_dataset/ \\
  --dataset.episode_time_s=20 \\
  --dataset.num_episodes=1 \\
  --policy.path=${HF_USER}/act_${DATASET_NAME} \\
  --dataset.push_to_hub=false
\`\`\`

## Training Tips

1. **Batch Size**: Adjust based on GPU memory
   - MI300X: 64-128 for ACT, 32-64 for VLA
   - Reduce if you get OOM errors

2. **Training Steps**: 
   - Start with 10,000 steps for testing
   - Increase to 50,000-100,000 for production

3. **Monitoring**:
   - Use Weights & Biases for real-time monitoring
   - Check training loss curves
   - Validate on held-out episodes

4. **Model Selection**:
   - **ACT**: Fast, good for simple tasks
   - **VLA**: Better for complex, language-conditioned tasks
   - **Pi**: Good for smooth, continuous motions

## Resources

- [LeRobot Documentation](https://huggingface.co/docs/lerobot/index)
- [Training Notebook](../training-models-on-rocm.ipynb)
- [Dataset](https://huggingface.co/datasets/${DATASET_REPO_ID})

---

**Generated by**: so101_scripts/scripts/09_prepare_training.sh
EOF

log_success "Training commands generated: $OUTPUT_FILE"
echo ""

# Display summary
log_info "Summary:"
log "  Dataset: $DATASET_REPO_ID"
log "  Training commands: $OUTPUT_FILE"
echo ""
log_info "Next steps:"
log "  1. Review the training commands in $OUTPUT_FILE"
log "  2. Copy commands to your cloud training environment"
log "  3. Start training on MI300X GPU"
log "  4. Monitor training with Weights & Biases"
echo ""

echo "=============================================="
echo ""

exit 0